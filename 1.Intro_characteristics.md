1. Time series in R: Introduction and characteristics
================

## Introduction, datasets and its types.

In this repository I want to explore various techniques and aspects of
time series analysis. I am writing these notes, while learning about
time series analysis My main source is the book ‘TIme Series Analysis
and Its Applications With R Examples’ by R.H.Shumway and D.S.Stoffer.  
This is the first notebook, where I will explore various datasets, check
and discuss their basic properties.

As a first glance, here are some of the datasets from astsa package that
will be utilised:  
1. Quarterly earnings of Johnson and Johnson,  
2. Global temperature deviations (base period for average: 1951-1980),  
3. .1 second sample of recorded speech for the phrase aaa…hhh.  

The first step of investigation of the data should always be plotting
the series over time, thus below we can take a look on the plotted
series. Autoplot() is a function from ggplot2 package that will
automatically plot time series for Object with ‘ts’ type.
![](some_basic_approaches_files/figure-gfm/unnamed-chunk-2-1.png)<!-- -->

The datasets come from the astsa library, they are $ts$ object, which is
specific format in R for handling regular time series data. It is
particularly well-suited if:

- data is regularly spaced (e.g., monthly, quarterly, annually), 
- you plan to use time series-specific functions like auto.arima(),
  HoltWinters(), or other built-in R time series functions (e.g.,
  plot.ts(), decompose(), forecast()), 
- you want to easily access properties like start date, end date, and
  frequency of the data.  

Another dataset that will be used to demonstrate the discussed
properties is the datase from Kaggle (CC0 license) describing airline
passenger data from year 1949 to 1960. To import the data I am using
read_csv from readr package as it handles big datasets better and
detects the type of data automatically. It is saved in the data.frame
type object. In general this format is more flexible and allows to store
multiple columns, thus it is more useful if:  

- data is irregularly spaced (e.g., daily data with gaps or missing
  values), 
- you need to store additional metadata or variables alongside your time
  series,   
- you plan to use ggplot2 or other tidyverse tools to visualize or
  manipulate the data, 
- you are working with multiple time series that need to be handled
  together.  

``` r
air <- readr::read_csv("AirPassengers.csv") %>%
  setNames(c('Month', 'Passengers'))
```

    ## Rows: 144 Columns: 2
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (1): Month
    ## dbl (1): #Passengers
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Although this is a regularly spaced and not really complex dataset, I
will continue to work on the data.frame object to see the differences.
For dealing with dates, the lubridate library comes in handy.

``` r
class(air$Month) #we see this is character so let's change into date with function ym() from lubridate
```

    ## [1] "character"

``` r
air$Date <- ym(air$Month)
ggplot(air, aes(x=Date, y=Passengers))+geom_line()+theme_minimal()+labs(title = 'Airline Passengers')
```

![](some_basic_approaches_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->

## Characteristics

In analysis of time series we can highlight two different approaches:  -
time domain approach, where we focus on lagged relationships in the
data, meaning: how does what happened influence what will happen in
futre,  - frequency domain approach, where we focus on cycles in the
series.  

In general we say, that time series is a stochastic process $\{X_t\}_t$
indexed by time. The observed value is some realization of this process.
Thus the real data we have observed is just one realization of this
process.  

### White noise $W_t$

It is a special time series generated from uncorrelated random variables
with mean 0 and finite constant variance $\sigma_w^2$. We will often use
special case of Gaussian white noise, where random variables are i.i.d.:
$W_t \sim N(0, \sigma_w^2)$.

### Moving average and filtering

When we observe high variability in the data, one way to smooth it to be
able to see some reguarity is fitrering the series. By this we mean,
taking a linear combination of neighbouring values, which is also called
moving average - as an arithemtic mean. 

Below I take a sample from Gaussian white noise series $W_t$ and then
calculate moving average $A_t$ - taking arithmetic mean of the series
itself and its neighbouring values:
$A_t = \frac{W_{t-1}+W_t+W_{t+1}}{3}$.

``` r
#white noise and its filtering: moving average
w = rnorm(500,0,1) # 500 N(0,1) variates
v = stats::filter(w, sides=2, filter=rep(1/3,3)) # moving average
par(mfrow=c(2,1))
plot.ts(w, main="white noise")
plot.ts(v, ylim=c(-3,3), main="moving average")
```

![](some_basic_approaches_files/figure-gfm/unnamed-chunk-5-1.png)<!-- -->
\### ACF Autocorrelation - correlation of the data with itself. In
theory it can be described as follows:

- autocovariavce:
  $\gamma(s,t) = cov(X_s, X_t) = \mathbb{E}[(X_s-\mu_s)(X_t-\mu_t)]$ 
- autocorrelation (ACF):
  $\rho(s,t) = \frac{\gamma(s,t)}{\sqrt{\gamma(s,s)\gamma(t,t)}}$, it
  always takes a value between -1 and 1. Measures linear predictability
  of series at time $t$ using only value at time $s$.  

We can also measure cross correlation between two different series.

In practice, when we have a time series dataset, we estimate
autocorrelation of lag k by the folowing formula:

$$r_k = \frac{\sum_{t=k+1}^N(y_t-\bar{y})(y_{t-k}- \bar{y})}{\sum_{t=1}^N(y_t-\bar{y})^2}$$
Interpretation: - if the correlation at certain lags multiplies
(e.g. every week) is higher than in the others it means there is a
seasonal component (here: weekly), - if the correlation for recent lags
is higher and is slowly decreasing, then there is some trend in the
data. We want to difference to get stationarity.

We can calculate autocorrelation of a time series as follows with a
built-in function from stats library:

``` r
air %>%
  select(Passengers)%>%
  acf(lag.max = 50, main = 'Autocorrelation plot for number of passengers')
```

![](some_basic_approaches_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->
We can also explore cross correlation between two series as follows:

``` r
par(mfrow=c(3,1))
acf(soi, 48, main="Southern Oscillation Index")
acf(rec, 48, main="Recruitment")
ccf(soi, rec, 48, main="SOI vs Recruitment", ylab="CCF")
```

![](some_basic_approaches_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->

### PACF

Correlation between two random variables whilst controlling the effect
of other rvs.  
In the context of time series, it is the correlation between the serias
at two different lags, not considering the effect of any intermediate
lags. It has it’s use in estimation of number of autoregressive
components in fitting the ARIMA model.

``` r
air %>%
  select(Passengers)%>%
  pacf(lag.max = 50, main = 'Partial autocorrelation plot for number of passengers')
```

![](some_basic_approaches_files/figure-gfm/unnamed-chunk-8-1.png)<!-- -->
